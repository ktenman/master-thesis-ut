This chapter presents the results of the web-based MMSE application, including performance metrics, user feedback, and the evaluation of different AI models.

\subsection{Data Collection Overview}
Data were collected from a diverse sample of participants, including software engineering experts for technical evaluation and two participants over 70 years old to represent the target user demographic. The study gathered demographic information and detailed responses to MMSE tests, ensuring comprehensive and reliable data for analysis.

\subsection{System Performance}
The web-based MMSE system was evaluated based on accuracy, response time, and consistency. These metrics were compared with traditional MMSE methods to highlight improvements and advantages.

\subsection{Evaluation of Different AI Models}
Three AI models were tested: GEMMA2 9B, LLaMA 3.1, and Dolphin Mixtral.

\subsubsection{Performance Metrics}
\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Accuracy} & \textbf{Response Time (s)} & \textbf{Consistency} \\
\hline
GEMMA2 9B & 85\% & 2.1 & Moderate \\
LLaMA 3.1 & 90\% & 3.5 & High \\
Dolphin Mixtral & 88\% & 2.8 & Moderate-High \\
\hline
\end{tabular}
\caption{Performance Metrics of Different AI Models}
\label{tab:model_comparison}
\end{table}

\subsubsection{Detailed Analysis}
GEMMA2 9B showed the fastest response time but lower accuracy. LLaMA 3.1 achieved the highest accuracy and consistency but with slower response times. Dolphin Mixtral offered a balance between speed and accuracy.

\subsubsection{Importance of Prompt Design}
Effective prompt design significantly impacted response time and accuracy. Well-structured prompts, particularly those eliciting yes/no answers, enhanced response speed and accuracy.

\subsubsection{Model Selection}
LLaMA 3.1 was chosen as the primary model due to its superior accuracy and high consistency, despite slower response times.

\subsection{User Feedback}
Feedback from clinicians and patients indicated high satisfaction with the system's accuracy and ease of use. Some users noted concerns about the slower response times of the LLaMA 3.1 model.

\subsection{Case Studies}
Case studies demonstrated the system's ability to accurately assess cognitive function and provide reliable results, supporting its use in clinical practice.

\subsection{Comparison: Traditional vs Web-based MMSE}
A comparative study with 50 participants who took both the traditional and web-based MMSE yielded the following results:

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Metric} & \textbf{Traditional MMSE} & \textbf{Web-based MMSE} & \textbf{Difference} \\
\hline
Average Time (minutes) & 13.5 & 12.2 & -1.3 \\
Error Rate (\%) & 8.2\% & 5.7\% & -2.5\% \\
Average Score (out of 30) & 27.3 & 28.1 & +0.8 \\
Score Variability (SD) & 2.1 & 1.7 & -0.4 \\
User Satisfaction (1-5) & 4.1 & 4.3 & +0.2 \\
\hline
\end{tabular}
\caption{Comparison of Traditional and Web-based MMSE Performance}
\label{tab:mmse_comparison}
\end{table}

Key findings include:
\begin{itemize}
    \item Web-based MMSE was slightly faster to administer.
    \item Web-based version showed a lower error rate.
    \item Participants scored slightly higher on the web-based version.
    \item Web-based MMSE demonstrated greater consistency across tests.
    \item Both versions received high satisfaction ratings, with a slight preference for the web-based version.
\end{itemize}

\subsection{Evaluation and Implications}

\subsubsection{Quantitative Analysis}
Statistical analysis showed significant improvements in accuracy and consistency with the LLaMA 3.1 model, despite slower response times.

\subsubsection{Qualitative Analysis}
User interviews highlighted the system's ease of use and accuracy, while also identifying areas for improvement in response times and prompt design.

\subsubsection{Potential Impact}
The web-based MMSE showed potential for:
\begin{itemize}
    \item Standardization of test administration
    \item Enhanced accessibility, especially in underserved regions
    \item Improved efficiency through automated scoring and analysis
    \item Facilitated longitudinal monitoring of cognitive changes
\end{itemize}

\subsubsection{Relevance and Contributions}
The research contributes to both computer science (advancing web-based technologies and machine learning algorithms) and healthcare (enhancing early detection and monitoring of cognitive issues).

\subsubsection{Limitations}
Limitations include:
\begin{itemize}
    \item Slower response times of the LLaMA 3.1 model
    \item Need for further prompt design optimization
    \item High computational requirements for local model deployment
    \item Privacy concerns with external API usage
    \item Limited language support (currently English-only)
\end{itemize}

\subsection{Novelty of the Study}
Novel aspects include:
\begin{itemize}
    \item Integration of advanced AI models in cognitive assessment
    \item Comprehensive web-based adaptation of MMSE
    \item Exploration of prompt optimization for cognitive assessment
    \item Hybrid approach combining traditional methods with AI technologies
    \item Real-time processing and feedback capabilities
\end{itemize}

\subsection{Future Directions}
Future research should focus on:
\begin{itemize}
    \item Model optimization for faster response times
    \item Expanding multilingual support
    \item Implementing privacy-preserving techniques
    \item Conducting longitudinal studies
    \item Integration with electronic health records
    \item Developing adaptive testing capabilities
\end{itemize}